---
title: "Research: Case-Cohort Study Design"
author: "Ziyuan Wang"
date: 2020-01-03T21:13:14-05:00
categories: ["R"]
tags: ["R Markdown", "plot", "regression"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(rje) #for expit
library(survival)
library(survey)
library(MASS)
library(mvtnorm)
```

## Background

In many epidemiological and disease prevention trials, the cost for getting covariate information from the full cohort is expensive. The case-cohort design is a study scheme proposed by Prentice back in 1986. The purpose of such design is to select a specific group of patients from the full cohort and get inferences from the selected sub-cohort instead of the full cohort. As you can see on the slide, the picture represents such a selection. That larger eclipse represents the full cohort. And we will be collecting Covariate information from the selected sub-cohort and for all cases outside of the sub-cohort . As Prentice pointed out, if the sub-cohort is selected wisely, the statistical power of the association between predictor of interest and endpoints will not be reduced, comparing to the power of the association obtained from analyzing the full cohort.

We want to implement this study scheme to HIV prevention settings, and utilize the case-cohort (two-phase) sampling schemes to study the predictors of interest and/or their interaction with the main intervention. However, the probability of exposure to HIV infected patients can vary across the population, and result in a large proportion of sampled participants never getting exposed. This may results in diluted estimates of the association for predictors of interest. In order to approach this question, we tried different sampling methods found in literature and ran simulation on the various methods.

# The simulation found on paper by Kim and De Gruttola

We first tried to replicate the simulation posted in "Strategies for cohort sampling under the cox proportional hazards model, application to an AIDS clinical trial", by Kim and De Gruttola. A sample size of 1000 was selected to resemble ACTG 019, a phase III clinical trial. Here, we have two covariates, Z1 and Z2, that follows a bivariate normal distribution with mean equal to (0,0)' and both variances equals to 1, and correlation equals to 0.5. Failure times were generated according to an expoential distribution with hazard rho0(exp(Z1beta1+Z2beta2)). Some example covariates under this circustance could be: CD4 cell count and age. The first covariate Z1 is observed for the entire cohort, and Z2, in theory, is observed only for the failures and the selected controls. The cox regression parameters beta 1 and beta 2 were set to 1 and 0.5 respectively, and rho0 to 0.1. Times of entry into the study were generated by a uniform (0,1) distribution, hence time of censoring is also uniform (0,1). In the reference paper, the average number of failures was approximately 91, whereas in this simulation, the average number of failures was 91. For the case-cohort designs, sampling method was to obtain approximately one control per failure. No stratification or weighting method was used, and the parameter estimates were obtained by applying the Self and Prentice method.
```{r, eval = FALSE}
set.seed(1009)
# creating single function to get single failure count
failure.times <- function(n=1000,mu1=0,mu2=0,s1=1,s2=1,rho=0.5,beta1=1,beta2=0.5,rho0=0.1) {
#n <- 1000
##create bivariate normal distribution for Z1 and Z2, with mean=(0,0)', var=1 and cor = 0.5
#mu1=mu2=0
#s1=s2=1
#rho <- 0.5
mu <- c(mu1,mu2) # Mean
sigma <- matrix(c(s1^2, s1*s2*rho, s1*s2*rho, s2^2),
           2,2) # Covariance matrix
bvn1 <- mvrnorm(n, mu,sigma, tol = 1e-6, empirical = FALSE, EISPACK = FALSE)
colnames(bvn1) <- c("Z1","Z2")
## generate failure times
#beta1 <- 1
#beta2 <- 0.5
#rho0 <- 0.1
vals <- as.data.frame(bvn1)
rate <- rho0*(exp(vals$Z1*beta1+vals$Z2*beta2))
f.r <- rexp(1000, rate)
## time of entry into the study
t.e <- runif(n, min = 0, max = 1)
## time of censoring
t.c <- runif(n, min = 0, max = 1)
failures <- t.c > f.r
time.to.failure <- t.e+f.r
sum(failures, na.rm = TRUE)}
# testing the results
failure.times(n=1000,mu1=0,mu2=0,s1=1,s2=1,rho=0.5,beta1=1,beta2=0.5,rho0=0.1)
# use a wrapper for average failure times
ave.failure.times <- function(z){
  sep.counts <- replicate(z, failure.times(n=1000,mu1=0,mu2=0,s1=1,s2=1,rho=0.5,beta1=1,beta2=0.5,rho0=0.1)) 
  sum(sep.counts)/z
}
# testing the result, compare it with 91 (reference number)
f.times <- ave.failure.times(1000)
```

# The Simulation Function
Building our own simulation dataset.
```{r simulation, eval = FALSE}
bv.simulation.2 <- function(exp.rate, beta, gamma, baseline.hazards, epsilon) {
# exp.rate is the exposure rate to HIV
# beta is the coefficient (intercept) in generating BV status
# gamma is the coefficient in generate BV status, and varied to control association
# of HIV exposure and BV status
  ########### main body #############
# generate sites and participants, sites 1:10, participants per site 1:250
sites <- rep(1:10, each = 250)
participants <- 1:2500
# generate exposure rates for study sites
# a.set <- seq(exp.rate-0.16,exp.rate+0.20, 0.04)
# calculate hiv exposure for each site
HIV.exp <- rbinom(2500,1,exp.rate) # varying by site as the original simulation plan
# generate bv status
bv <- rbinom(2500,1,expit(beta + gamma*HIV.exp))# correlation with HIV exp varies, increase gamma and cut beta
# generate the intervention assignment
# tx <- rep(c(0,1),times = 125)
# generate the hazards
# overall HIV incidence of 5% per year
individual.hazards <- baseline.hazards*exp(epsilon*bv) # only epsilon
# set those whose exposure status of 0 to small hazard
individual.hazards.2 <- ifelse(HIV.exp==0, 0.00001, individual.hazards)
Time.to.HIV <- rexp(2500, individual.hazards.2)
# combine and arrange the time to hiv
sort.time <- sort(Time.to.HIV)
sort.time[160]
# add numbering to the dataset
# set the time to event to NA if  exceed 160 events (study ends when reach 160 events)
Time.to.HIV <- ifelse(Time.to.HIV <= sort.time[160], Time.to.HIV, sort.time[160]+1)
# identify those top 160 time to events and label them with HIV infection
HIV.infection <- ifelse(Time.to.HIV <= sort.time[160], 1, 0)
data.frame(sites,participants,HIV.exp,bv,individual.hazards.2,Time.to.HIV,HIV.infection)
} # wait until the end to combine the data
# test the function
bv.sim.2 <- bv.simulation.2(0.4, -1.5, 2.5, 0.0001405205, 0)

bv.sim.2 %>% summarise(prop = mean(bv)) %>%
                    .$prop  # should be close to 40%
bv.sim.2 %>% summarise(prop = mean(HIV.exp)) %>%
                    .$prop # should be close to 40%
```

# Implementing the survey package: Lin & Ying
```{r, eval = FALSE}
# simple random
rep.svy.LY <- function(hiv.exp, beta, gamma) {
bv.sim.2 <- bv.simulation.2(hiv.exp, beta, gamma, 0.0001405205, 0)

sel.sub <- sample(bv.sim.2$participants,250,replace = F)
bv.sim.2$sel.sub <- ifelse(bv.sim.2$participants %in% sel.sub, 1, 0)

dcch<-twophase(id=list(~participants,~participants), strata=list(NULL,~HIV.infection),
                  subset=~I(sel.sub | HIV.infection), data=bv.sim.2)
fit1 <- svycoxph(Surv(Time.to.HIV,HIV.infection)~bv+strata(sites),
                design=dcch)
# probabilities
rates <- rep(0,10)
  for (i in 1:10) {
  rates[i] <- bv.sim.2 %>% filter(sites == i) %>%
  summarise(prop = mean(HIV.infection)) %>%
                    .$prop
  }
  HIV.probs.list <- rep(rates, each = 250)# tabulate
  indexes.proportion <- sample(bv.sim.2$participants, size = 250, prob = HIV.probs.list, replace =   FALSE)
  bv.sim.2$site.sel.proportion <- ifelse(bv.sim.2$participants %in% indexes.proportion, 1, 0)

dcch.site1<-twophase(id=list(~participants,~participants), strata=list(NULL,~HIV.infection),
                  subset=~I(site.sel.proportion | HIV.infection), data=bv.sim.2)
fit2 <- svycoxph(Surv(Time.to.HIV,HIV.infection)~bv+strata(sites),
                design=dcch.site1)
# logistic
  logit1 <- glm(HIV.infection~as.factor(sites), data=bv.sim.2, family = "binomial")
  ## use the original method posted in the MORGAM paper to get the probability of getting infection
  bv.sim.2$Site.prob <- predict(logit1, newdata = bv.sim.2, type = "response")
  indexes.logit <- sample(bv.sim.2$participants, size = 250, prob = bv.sim.2$Site.prob[1:2500], replace = FALSE)
  # generating the dataset
  bv.sim.2$site.sel.logit <- ifelse(bv.sim.2$participants %in% indexes.logit, 1, 0)
dcch.site2<-twophase(id=list(~participants,~participants), strata=list(NULL,~HIV.infection),
                  subset=~I(site.sel.logit | HIV.infection), data=bv.sim.2)
fit3 <- svycoxph(Surv(Time.to.HIV,HIV.infection)~bv+strata(sites),
                design=dcch.site2)
matrix(c(fit1$coefficients,fit1$var,
         fit2$coefficients,fit2$var,
         fit3$coefficients,fit3$var), byrow = TRUE)
}
mat1 <- replicate(1000,rep.svy.LY(0.4, -1.5, 2.5))

convert_to_table <- function(dataset,n) {
first.set <- matrix(dataset, nrow = n, byrow = TRUE)
result1.mean <- apply(first.set, 2, mean)
table1 <- matrix(result1.mean, nrow = 3, byrow = TRUE)
colnames(table1) <- c("mean","variance")
rownames(table1) <- c("SRS","Baseline-adjusted 1","Baseline-adjusted 2")
table1
}
```
